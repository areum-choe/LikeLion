{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 주식정보 가져오기\n",
    "https://finance.naver.com/sise/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://finance.naver.com/sise/'\n",
    "page = urlopen(url)\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### id를 이용해서 정보를 가져올 수 있다.\n",
    "#### tag : span\n",
    "#### id : KOSPI_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kospi = soup.find('span', id='KOSPI_now').text\n",
    "kosdaq = soup.find('span', id='KOSDAQ_now').text\n",
    "kospi_200 = soup.find('span', id='KPI200_now').text\n",
    "\n",
    "print('현재 코스피 지수 :', kospi)\n",
    "print('현재 코스닥 지수 :', kosdaq)\n",
    "print('현재 코스피200 지수 :', kospi_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 코스피 200지수\n",
    "soup_p = soup.find('div', id='tab_sel3_risefall').find('dd', class_='profit').text\n",
    "list_soup = [i for i in soup_p.split('\\n') if i]\n",
    "\n",
    "print(list_soup[0]+'은', list_soup[1]+'입니다.')\n",
    "\n",
    "soup_p = soup.find('div', id='tab_sel3_risefall').find('dd', class_='loss').text\n",
    "list_soup = [i for i in soup_p.split('\\n') if i]\n",
    "\n",
    "print(list_soup[0]+'은', list_soup[1]+'입니다.')\n",
    "\n",
    "soup_p = soup.find('div', id='tab_sel3_risefall').find('dd', class_='total').text\n",
    "list_soup = [i for i in soup_p.split('\\n') if i]\n",
    "\n",
    "print(list_soup[0]+'는', list_soup[1]+'입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 코스닥 지수\n",
    "soup_p = soup.find('div', id='tab_sel2_risefall').find('dd', class_='profit').text\n",
    "list_soup = soup_p.split('\\n')\n",
    "\n",
    "print(list_soup[1]+'은', list_soup[2]+'입니다.')\n",
    "\n",
    "soup_p = soup.find('div', id='tab_sel2_risefall').find('dd', class_='loss').text\n",
    "list_soup = soup_p.split('\\n')\n",
    "\n",
    "print(list_soup[1]+'은', list_soup[2]+'입니다.')\n",
    "\n",
    "soup_p = soup.find('div', id='tab_sel2_risefall').find('dd', class_='total').text\n",
    "list_soup = soup_p.split('\\n')\n",
    "\n",
    "print(list_soup[1]+'는', list_soup[2]+'입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 코스피 지수\n",
    "soup_t = soup.find('dl', id='basis_trend').text\n",
    "list_soup_t = [i for i in soup_t.split('\\n') if i]\n",
    "\n",
    "print(list_soup_t[1]+'은', list_soup_t[2]+'입니다.')\n",
    "print(list_soup_t[3]+'은', list_soup_t[4]+'입니다.')\n",
    "print(list_soup_t[5]+'는', list_soup_t[6]+'입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "정리\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://finance.naver.com/sise/\"\n",
    "\n",
    "page = urlopen(url)\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "rank_1 = soup.find('div', class_='rgt')\n",
    "rank_1 = soup.findAll('ul', id='popularItemList')\n",
    "\n",
    "list_rank = rank_1[0].findAll('li')\n",
    "\n",
    "for i in list_rank:\n",
    "    code = i.find('a')\n",
    "    company = code.text\n",
    "    code = code.get('href')\n",
    "    print(company+'의 코드 : ', code[-6:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Top 종목에 있는 정보를 sub메뉴(상한가, 하한가 등) 정보를 가져와서 csv파일로 정리하기.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://finance.naver.com/sise/\"\n",
    "sub_url = \"https://finance.naver.com/item/main.nhn?code=\"   # 인기 종목의 각 url 공통부분\n",
    "comp_list = []\n",
    "code_list = []\n",
    "target_data3 = []\n",
    "total_info = []\n",
    "\n",
    "page = urlopen(url)\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "rank_1 = soup.find('div', class_='rgt')\n",
    "rank_1 = soup.findAll('ul', id='popularItemList')\n",
    "\n",
    "list_rank = rank_1[0].findAll('li')\n",
    "\n",
    "for i in list_rank:   ## 인기 검색 종목의 코드 추출\n",
    "    code = i.find('a').get('href')\n",
    "    company = i.find('a').text\n",
    "    comp_list.append(company)\n",
    "    code_list.append(code[-6:])    \n",
    "#     print(company, code[-6:])\n",
    "\n",
    "for k in range(len(comp_list)):  ## 인기 검색 종목 각각의 url에서 정보 추출\n",
    "    target_url = sub_url + str(code_list[k])\n",
    "    target_page = urlopen(target_url)\n",
    "    target_soup = BeautifulSoup(target_page, 'html.parser')\n",
    "    \n",
    "    ## 각 url의 정보 추출\n",
    "    target_data1 = target_soup.find('div', id=\"middle\", class_=\"new_totalinfo\").findAll('dd')\n",
    "    \n",
    "    ## target_data3 : 각 url의 11가지 정보를 리스트 형태로 저장한 것\n",
    "    for j in range(1, 12):   # 0~12 인덱스 : 정보\n",
    "        target_data2 = target_data1[j].text\n",
    "        target_data3.append(target_data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## target_data3의 각 인덱스의 정보를 목록과 값으로 분리\n",
    "c_1, c_2, c_3, c_4, c_5, c_6  = [], [], [], [], [], []\n",
    "c_7, c_8, c_9, c_10, c_11,  = [], [], [], [], []\n",
    "\n",
    "for j in target_data3:\n",
    "    target_data4 = j.split(' ')\n",
    "    target_data5 = ' '.join(target_data4[1:])\n",
    "    total_info.append(target_data5)   # total_info : 10개 종목 당 11개 정보 / 리스트 형태\n",
    "\n",
    "for j in range(0, 110, 11):   # 종목명\n",
    "    c_1.append(total_info[j])\n",
    "for j in range(1, 110, 11):   # 종목코드\n",
    "    c_2.append(total_info[j])\n",
    "for j in range(2, 110, 11):   # 현재가\n",
    "    c_3.append(total_info[j])\n",
    "for j in range(3, 110, 11):   # 전일가\n",
    "    c_4.append(total_info[j])\n",
    "for j in range(4, 110, 11):   # 시가\n",
    "    c_5.append(total_info[j])\n",
    "for j in range(5, 110, 11):   # 고가\n",
    "    c_6.append(total_info[j])\n",
    "for j in range(6, 110, 11):   # 상한가\n",
    "    c_7.append(total_info[j])\n",
    "for j in range(7, 110, 11):   # 저가\n",
    "    c_8.append(total_info[j])\n",
    "for j in range(8, 110, 11):   # 하한가\n",
    "    c_9.append(total_info[j])\n",
    "for j in range(9, 110, 11):   # 거래량\n",
    "    c_10.append(total_info[j])\n",
    "for j in range(10, 110, 11):   # 거래대금\n",
    "    c_11.append(total_info[j])\n",
    "    \n",
    "# print(c_1)\n",
    "# print(c_2)\n",
    "# print(c_3)\n",
    "# print(c_4)\n",
    "# print(c_5)\n",
    "# print(c_6)\n",
    "# print(c_7)\n",
    "# print(c_8)\n",
    "# print(c_9)\n",
    "# print(c_10)\n",
    "# print(c_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = {'종목명':c_1, '종목코드':c_2, '현재가':c_3 , '전일가':c_4 , '시가':c_5 ,\n",
    "       '고가':c_6 , '상한가':c_7 , '저가':c_8 , '하한가':c_9 , '거래량':c_10 ,\n",
    "       '거래대금':c_11 }\n",
    "dat = pd.DataFrame(dat)\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.to_csv(\"인기종목 정보.csv\", index=False)\n",
    "\n",
    "## 확인\n",
    "import os\n",
    "print(os.getcwd())  # 현재 위치\n",
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 관심 있는 종목 5개 선정, 코드 확인 후, 내가 필요로 하는 정보 6개 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "comp_list = ['삼성전자', '카카오', '현대바이오', '셀트리온', 'NAVER']\n",
    "code_list = ['005930', '035720', '048410', '068270', '035420']\n",
    "list_1 = []\n",
    "\n",
    "base_url = 'https://finance.naver.com/item/main.nhn?code='\n",
    "\n",
    "for idx, code in enumerate(code_list):\n",
    "    all_url = base_url + code\n",
    "    page = urlopen(all_url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    \n",
    "    cur_price = soup.find('p', class_='no_today').span.text\n",
    "    am = soup.find('div', id=\"middle\", class_=\"new_totalinfo\").findAll('dd')\n",
    "    \n",
    "    print(comp_list[idx], cur_price, '\\n', am[10].text, '\\n', \n",
    "         am[3].text, '\\n', am[4].text, '\\n', am[5].text, '\\n', am[6].text, '\\n',\n",
    "         am[7].text, '\\n', am[8].text, '\\n', am[9].text)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
